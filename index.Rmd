---
title: Quantitative Genetics
subtitle: A Review of Statistics
author: Antonio Augusto F Garcia
mode : selfcontained
framework: revealjs
widgets : [mathjax]  # {mathjax, quiz, bootstrap}
hitheme : zenburn
revealjs:
  theme: night
  transition: linear
  center: "true"
url: {lib: "."}
bootstrap:
  theme: amelia
---

# Quantitative Genetics

## A Review of Statistics

<small>
Instructor: [A Augusto F Garcia](http://about.me/augusto.garcia)
/ [Statistical Genetics Lab](http://statgen.esalq.usp.br) </small>

<small>Department of Genetics, Luiz de Queiroz College of Agriculture,
University of São Paulo (Brazil)</small>

<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>

*** =pnotes

Some notes on the first slide

---

# Content

### Expectation
### Variance
### Covariance
### Regression
### Correlation

---

### Probability distribution

- We will denote $z$ for a random variable (quantitative trait,
  phenotype)

- $p(z)$ (probability density function) 


$\int_{-\infty}^{+\infty} p(z) dz = 1$

$P(z_1\leq z \leq z_2)=\int_{z_1}^{z_2}p(z) dz$

---

### Normal distribution

$$f(x)=\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{\frac{-(x-\mu)^2}{2\sigma^{2}}}$$

```{r,echo=F}
x <- seq(-3,3,length=1000)
P <- dnorm(x,mean=0,sd=1)
df <- data.frame(as.numeric(x), as.numeric(P))
g <- ggplot(df, aes(x, P)) + geom_line()
g
```

---

# Expectation

- First moment about the origin, **expected value**, expectation, mean

$$E(z)=\mu=\int_{-\infty}^{+\infty} z \, p(z) \, dz$$

- Properties

  - $E(x+y)=E(x)+E(y)$
  - $E(cx)=c E(x)$

---

# Variance

- Second moment about the mean

$$\sigma^2=\int_{-\infty}^{+\infty} (z-\mu)² \, p(z) \, dz=E[(z-\mu)^2]$$

$$\sigma² = E(z^2)-[E(z)]^2$$

If $z$ are deviations around the mean (e. g. after scaling)

$\sigma^2=E(z^2)$

---


## Joint Distribution


$$P(y_1 \leq y \leq y_2, \, x_1 \leq x \leq x_2) =
\int_{y_1}^{y_2}\int_{x_1}^{x_2} p(x,y) \, dx\, dy$$


- Conditional probabilities
$$P(y_1 \leq y \leq y_2 | x) = \int_{y_1}^{y_2} p(y|x) \, dy$$

- Remember that
$$p(x,y)=p(y|x) \, p(x)$$

- Expectation
$$E[f(x,y)]=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}
f(x,y)\,p(x,y)\,dx \, dy$$

---

# Covariance

$$\sigma(x,y)=E[(x-\mu_x)(y-\mu_y)]$$

$$\sigma(x,y)=E(xy)-\mu_x \mu_y=E(xy)-E(x)\,E(y)$$


---

### Covariance

- Measure of **linear association** between $x$ and $y$

```{r, echo=FALSE}
x  <- rnorm(100, mean = 0, sd = 1)
y  <- x + rnorm(100, 0, 10)
d <- data.frame(x,y)
g <- ggplot(d, aes(x=x, y=y)) + geom_point()
g
```

---

## Useful Identities

$a$ is a constant, $x$ and $y$ are random variables

- $\sigma(x,x)=\sigma^2(x)$
- $\sigma(a,x)=0$
- $\sigma(ax,y)=a \, \sigma(x,y)$
- $\sigma(ax,by)=ab \, \sigma(x,y)$
- $\sigma^2(ax)=a^2\sigma^2(x)$
- Very important:
$$\sigma^2(x+y)=\sigma^2(x)+\sigma^2(y)+2\sigma(x,y)$$

---

# Regression



---

# Homework

  - Read Chapter 1 of Lynch and Walsh Book
